---
title: "Distributed Ledger Technology (DLT)"
categories: 
  - Data
tags: 
  - Software
authors:
  - 0xthiagomartins
  - samuelpoiani
date: 2024-06-02
---
# Data

In common usage, data are a collection of discrete or continuous values that convey information, describing the quantity, quality, facts, statiscs, other **basic units of meaning**, or simply sequences of symbols that may be further interpreted formally.

A **datum** is an individual value value in a collection of data. Data are usuallyorganized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures. 

Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements. Data are commonly used in scientific research, economics, and in virtually every other form of human organizational activity. Examples of data sets include price indices (such as consumer price index), unemployment rates, literacy rates, and census data. In this context, data represent the raw facts and figures from which useful information can be extracted.

## Information and Knowledge

Thematically connected data presented in some relevant context can be viewed as *information*. Contextually connected pieces of information can then be described as *data insights or intelligence*. The stock of information, can then be described as *knowledge*. Data have been described as "the new oil of the digital economy".

Data, information, knowledge and wisdom are closely related concepts, but each has its role concerning the other, and each term has its meaning. According to a common view, data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. One can say that the extend to which a set of data is informative to someone depends on the extent to which it is unexpected by that person. The amount of information contained in a data stream may be characterized by its Shannon entropy

Knowledge is the awareness of its environment that some entity possesses, whereas data merely communicate that knowledge. For example, the entry in a  database specifying the height of Mount Everest is a datum that communicates a precisely-measured value. This measurement may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it. An awareness of the characteristics represented by this data is knowledge.

Data is often assumed to be the least abstract concept, information the next least, and knowledge the most abstract. In this view, data becomes information by interpretation; e.g., the height of Mount Everest is generally considered "data", a book on Mount Everest geological characteristics may be considered "information", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered "knowledge". "Information" bears a diversity of meanings that ranges from everyday usage to technical use. This view, however, has also been argued to reverse how data emerges from information, and information from knowledge.

Generally speaking, the concept of information is closely related to notions of constraint, communicator, control, data, form, instruction, knowledge, meaning, mental, mental stimulus, pattern, perception, and representation. Beynon-Davies uses the concept of a sign to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.

---

Before the development of computing devices and machines, people had to manually get data and impose patterns on it. Since the development of computing devices and machines, these devices can also get data. In the 2010s, computers are widely used in many fields to get data and sort or process it, in disciplines ranging from marketing, analysis of social services usage by citizens to scientific research. These patterns in data are seen as information that can be used to enhance knowledge. These patterns may be interpreted as "truth" (though "truth" can be a subjective concept) and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken.

---
## Types

Data can be broadly classified into two main types: qualitative and quantitative.

1. **Qualitative/Categorical**: Non-numeric data that describes attributes or properties. Examples include names, labels, and descriptions.
   - **Ordinal**: Categorical data with a meaningful order or ranking but no fixed interval between categories.
   - **Nominal**: Categorical data without any inherent order. It labels variables without a quantitative value.
2. **Quantitative**: Numeric data that can be measured and quantified. Examples include height, weight, and temperature.
   - **Discrete**: Consists of distinct, separate values. It often represents counts and cannot be subdivided meaningfully.
   - **Continuous**: Can take any value within a range. It represents measurements and can be divided into finer levels.

| ![Data Types](../assets/imgs/data_type_diagram.png) |
| :--: |
| *Data Types* |

## Meta-data

The concept of data about data is known as meta-data. It is also useful to distinguish meta-data, that is, a description of other data. A similar yet earlier term for meta-data is "ancillary data". The prototypical example of meta-data is the library catalog, which is a description of the contents of books. It includes:

- **Descriptive** Meta-data: Information describing the content, such as the title, author, and keywords.
- **Structural** Meta-data: Information about the format and structure of the data, such as file types and formats.
- **Administrative** Meta-data: Information about the creation, management, and use of the data, such as copyright details and data provenance

## Representation

Mechanical computing devices are classified according to how they represent data. 
- An **analog** computer represents a datum as a voltage, distance, position, or other physical quantity. 
- A **digital** computer represents a piece of data as sequence of symbols drawn from a fixed alphabet. The most common digital computers use a binary alphabet, that is, an alphabet of two characters typically denoted "0" and "1".

More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A computer program is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably Lisp and similar languages, programs are essentially indistinguishable from other data.

# Challenges in a data system

As a reference architecture is abstract and designed with generality in mind, it is applicable in different contexts, where the concrete requirements of each context guide the adoption into a concrete architecture. The level of abstraction can however differ between reference architectures and with it the concreteness of guidance a reference architecture can offer

## Acquisition (Collection and/or Extraction)

Data are acquisited using techniques such as measurement, observation, query or analysis, and are typically represented as numbers or characters which may be further processed.

Gathering data can be accomplished through a primary source (the research is the first person to obtain the data) or a secondary source (the researcher obtains the data that has already been acquisited by other sources, such as data disseminated in a scientific journal).

1. **Field data** are data that are gathered in an uncontrolled in-situ enviroment.
2. **Experimental data** are data that are generated in the course of a controlled scientific experiment.

We have two types of the data acquisition:

1. **Collection** is the acquisition of data in **motion**. The acquisition includes capabalities to "subscribe" to a data stream or to simulate the stream via quickly repeating API calls. if it is subscribed then data will be pushed  to it. Once the data is collected, it may then store it temporarily in-memory or a disk before passing it to Stream processor for near real-time processing of data. Data can also go through filtration before it being fed to the Stream processor. Data Collection aims at acquiring data in motion.

2. **Extraction** is the acsition of data **at rest**. It is primarily a pull mechanism to feed the data into a system. It can also refer to crawling data from web pages. Data can be extracted from structured, semi-structured and unstructured data sources. Data extraction aims at acquiring static data.

---
Data are analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis. Prior to analysis, raw data (or unprocessed data also known as primary data) are typically cleaned: Outliers are removed and obvious instrument or data entry errors are corrected.

Data analysis methodologies vary and include data triangulation and data percolation. The latter offers an articulate method of acquiring, classifying, and analyzing data using five possible angles of analysis (at least three) to maximize the research's objectivity and permit an understanding of the phenomena under investigation as complete as possible: qualitative and quantitative methods, literature reviews (including scholarly articles), interviews with experts, and computer simulation. The data is thereafter "percolated" using a series of pre-determined steps so as to extract the most relevant information.

---

## Processing

Data processing is the execution of a function that receives a given data set as input, and that function will perform an action and/or return another data set as output.

The data processing infrastructure that was developed to run business operations over the past decades is having trouble keeping pace with today's digital alndscape. Across every industry daily interactions and transactions are moving online and the business services are becoming increasingly automated. Volumes of multi-structured, machine-generated data from a variety of sources have skyrocketed,  and smart companies want to capture and make use of it all. As data volumes grow and the complexity of data types and sources increases, data processing workloads take longer to run and the time available and analysis is reduced.

The magnitude of 3Vs (Volume, Velocity and Variety) determines the computation power required to process data. Imagine the computational power required to process 20 petabytes of data within a day by Google, or analyzing 200 million tweets per day to derive trending topics on Twitter, or NYT processing 4TB worth of raw images into 11 Million finished PDFs in 24 hrs. So the challenge is not only because of tremendous volume of data, but also the speed (velocity) at which it is generated and the variety of sources from which it is produced.

Various traditional technologies are available for high-perfromance computing which includes Cluster COmputing and Grid Computing. However, high cost of Cluster and non-reliability of Grid Computing make them unsuitable for Data processing. Cloud computing which though seems to be an ideal solution for Data processing too puts up constraints such as the need to transfer huge volumes of data across network.

The challenges for high-performance computing is on following parameters:

- **Linear Horizontal Scalability:** ;
- **Coste Effectiveness:** ;
- **Performance:** ;
- **Minimum data transfer:** ;
- **Fault tolerance:** ;
- **Auto Elasticity:** ;
- **Latency:** .

## Storage

Companies today store large volumes of diverse data from web logs, click streams, sensors, and many other sources. The volume of this data crosses many terabytes and petabytes at times. Technically it is possible to store this amount of data on disks, however, challenge lies in cost effective means of data storage.

The various types of data storage provided in Data Store are as following

- Distributed File System
- RDBMS
- NoSQL
- In-Memory Databases (NewSQL)
- Encrypted Databases

A Data storage solution should overcome the following challenges

- **Preventing data loss:** a
- **Maintain High Availability of System:** a
- **Protecting data from unathorized access:** a
- **Managing Economy of Scale:** a

## Security and Privacy
## Governance and Management

## Big Data

Advances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible.

Doug Laney attribute 3Vs to data. They are Velocity, Volume and Variety. In other words high Volume, Velocity and Variety (3 Vs) are the attributes of a data that characterize it into a Big Data.

![Big Data 3 Vs](../assets/imgs/big_data_3v.png)

Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.

In response, the relatively new field of data science uses machine learning (and other artificial intelligence - AI) methods that allow for efficient applications of analytic methods to big data.


# Data Structure
# Data Integration


{{ footer(page.meta) }}